{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M3zQEQy9Gsc"
   },
   "source": [
    "# 3D UNet training for stroke detection in MRI\n",
    "* one subject less than in ATLAS R2.0 - upload problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GV0gF_jW_TpJ"
   },
   "source": [
    "## Importing libraries, dividing annotated data into training, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4575,
     "status": "ok",
     "timestamp": 1668765985669,
     "user": {
      "displayName": "Ondřej Nantl",
      "userId": "00763901182080487698"
     },
     "user_tz": -60
    },
    "id": "tPGT2sV628Uz",
    "outputId": "156e2cba-ceb1-46b4-c84e-360f10f1e5d6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from sklearn.utils.class_weight import compute_class_weight \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math,os,sys\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from pdb import set_trace\n",
    "\n",
    "# then import my classes\n",
    "from loaders import ISLES_Dataset\n",
    "from res3dunet import Res3DUNet\n",
    "from unet3d import UNet3D\n",
    "from loss_fcns import TverskyLoss,CrossEntropyDiceLoss,DiceLoss\n",
    "\n",
    "# device for pytorch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjpQUN39ndmw"
   },
   "source": [
    "## Code for generating the dataset lists\n",
    "This cell will produce 3 lists of folder names for training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tUNBBRAndmx"
   },
   "outputs": [],
   "source": [
    "# get folder content of annotated data\n",
    "imgPath = r'/mnt/Data/ondrejnantl/DPData/train/derivatives/ATLAS'\n",
    "imgAnnot = sorted(os.listdir(imgPath))\n",
    "imgAnnot.remove(imgAnnot[0])\n",
    "\n",
    "valCount = math.floor(0.15*len(imgAnnot))\n",
    "testCount = math.floor(0.25*len(imgAnnot))\n",
    "trainCount = len(imgAnnot) - (valCount+testCount)\n",
    "\n",
    "randIdx = random.sample(imgAnnot,len(imgAnnot))\n",
    "\n",
    "# splitting subjects into training, validation and testing part\n",
    "valIdx = randIdx[0:(valCount)]\n",
    "testIdx = randIdx[valCount:(valCount+testCount)]\n",
    "trainIdx = randIdx[(valCount+testCount):len(imgAnnot)+1]\n",
    "\n",
    "print('ID lists created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jspwId-pTOzH"
   },
   "source": [
    "This cell will produce 3 lists of folder names for training, validation and test datasets. It uses only cohorts R009 cohort of ATLAS v2.0 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ddau5ZSKTOzH",
    "outputId": "3c26b34e-3b75-4b96-faf1-afa5bd76f92a"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# get folder content of annotated data\n",
    "imgPath = r'/mnt/Data/ondrejnantl/DPData/train/derivatives/ATLAS'\n",
    "imgAnnot = sorted(os.listdir(imgPath))\n",
    "imgAnnot.remove(imgAnnot[0])\n",
    "\n",
    "# defining cohort to use for training\n",
    "cohorts = [\"^sub-r009.*$\"]\n",
    "selImgAnnot = []\n",
    "for it in cohorts:\n",
    "    r = re.compile(it)\n",
    "    tempList = list(filter(r.match, imgAnnot))\n",
    "    selImgAnnot = selImgAnnot + tempList\n",
    "\n",
    "valCount = math.floor(0.15*len(selImgAnnot))\n",
    "testCount = math.floor(0.25*len(selImgAnnot))\n",
    "trainCount = len(selImgAnnot) - (valCount+testCount)\n",
    "\n",
    "randIdx = random.sample(selImgAnnot,len(selImgAnnot))\n",
    "\n",
    "# splitting subjects into training, validation and testing part\n",
    "valIdx = randIdx[0:(valCount)]\n",
    "testIdx = randIdx[valCount:(valCount+testCount)]\n",
    "trainIdx = randIdx[(valCount+testCount):len(selImgAnnot)+1]\n",
    "\n",
    "print('ID lists created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Roonsfeindmx"
   },
   "source": [
    "## Load datasets' lists\n",
    "Loading object IDs from division of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxYXe14Rndmx"
   },
   "outputs": [],
   "source": [
    "imgPath = r'/mnt/Data/ondrejnantl/DPData/train/derivatives/ATLAS'\n",
    "\n",
    "trainFd= open(r'/mnt/Data/ondrejnantl/DPData/trainNames.txt', 'r')\n",
    "trainIdx = trainFd.read()\n",
    "trainIdx = trainIdx.splitlines()\n",
    "trainFd.close()\n",
    "\n",
    "valFd= open(r'/mnt/Data/ondrejnantl/DPData/valNames.txt', 'r')\n",
    "valIdx = valFd.read()\n",
    "valIdx = valIdx.splitlines()\n",
    "valFd.close()\n",
    "\n",
    "testFd= open(r'/mnt/Data/ondrejnantl/DPData/testNames.txt', 'r')\n",
    "testIdx = testFd.read()\n",
    "testIdx = testIdx.splitlines()\n",
    "testFd.close()\n",
    "\n",
    "print('IDs loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLFRG4WpTOzK"
   },
   "source": [
    "Load subjects from cohort R009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGkPahgxTOzL",
    "outputId": "41be1134-80f5-4265-f3c6-a2e180be9083"
   },
   "outputs": [],
   "source": [
    "imgPath = r'/mnt/Data/ondrejnantl/DPData/train/derivatives/ATLAS'\n",
    "\n",
    "trainFd= open(r'/mnt/Data/ondrejnantl/DPData/trainNamesSP.txt', 'r')\n",
    "trainIdx = trainFd.read()\n",
    "trainIdx = trainIdx.splitlines()\n",
    "trainFd.close()\n",
    "\n",
    "valFd= open(r'//mnt/Data/ondrejnantl/DPData/valNamesSP.txt', 'r')\n",
    "valIdx = valFd.read()\n",
    "valIdx = valIdx.splitlines()\n",
    "valFd.close()\n",
    "\n",
    "testFd= open(r'/mnt/Data/ondrejnantl/DPData/testNamesSP.txt', 'r')\n",
    "testIdx = testFd.read()\n",
    "testIdx = testIdx.splitlines()\n",
    "testFd.close()\n",
    "\n",
    "print('IDs loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CXTTp9w-Ved",
    "tags": []
   },
   "source": [
    "## Get dataset for 3D\n",
    "Create Pytorch dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1668765992781,
     "user": {
      "displayName": "Ondřej Nantl",
      "userId": "00763901182080487698"
     },
     "user_tz": -60
    },
    "id": "rki_SB1r-C-0",
    "outputId": "7702e087-c716-4298-918e-a9a93a3ab339"
   },
   "outputs": [],
   "source": [
    "tr_ds = ISLES_Dataset(\n",
    "    imgPath= imgPath,\n",
    "    imgAnnot = trainIdx,\n",
    "    cropped = True,\n",
    "    downsample = True\n",
    "    )\n",
    "tr_ds_dl = torch.utils.data.DataLoader(tr_ds,batch_size=4,shuffle=True)\n",
    "\n",
    "val_ds = ISLES_Dataset(\n",
    "    imgPath= imgPath,\n",
    "    imgAnnot = valIdx,\n",
    "    cropped = True,\n",
    "    downsample = True\n",
    "    )\n",
    "val_ds_dl = torch.utils.data.DataLoader(val_ds,batch_size=4,shuffle=False)\n",
    "\n",
    "ts_ds = ISLES_Dataset(\n",
    "    imgPath=imgPath,\n",
    "    imgAnnot=testIdx,\n",
    "    cropped = True,\n",
    "    downsample = True\n",
    "    )\n",
    "ts_ds_dl = torch.utils.data.DataLoader(ts_ds,batch_size=4,shuffle=False)\n",
    "\n",
    "print('Datasets and dataloaders created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqqS36SQAWHe"
   },
   "source": [
    "## Training the net and validation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "executionInfo": {
     "elapsed": 1397084,
     "status": "error",
     "timestamp": 1668767395100,
     "user": {
      "displayName": "Ondřej Nantl",
      "userId": "00763901182080487698"
     },
     "user_tz": -60
    },
    "id": "3hL4sNBVAWph",
    "outputId": "0ec5cc2d-b160-4d7b-98db-48fca25f135e"
   },
   "outputs": [],
   "source": [
    "#%% U-Net\n",
    "net = Res3DUNet(1,2,filters=[32,64,128,256])\n",
    "# net = UNet3D(1,2,filters=[32,64,128,256])\n",
    "net.to(device)\n",
    "\n",
    "#%% training and validation\n",
    "# loss_f = torch.nn.CrossEntropyLoss(weight = torch.Tensor([0.003, 0.997]).to(device))\n",
    "# loss_f = CrossEntropyDiceLoss(weights=[0.003,0.997])\n",
    "loss_f = DiceLoss()\n",
    "# loss_f = TverskyLoss()\n",
    "\n",
    "net.eval()\n",
    "opt = torch.optim.Adam(net.parameters(),lr=6e-5,weight_decay = 5e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=15, gamma=0.1)\n",
    "\n",
    "tr_losses = []\n",
    "val_losses = []\n",
    "tr_dice_scores = []\n",
    "dice_scores = []\n",
    "best_net = []\n",
    "learning_rates = []\n",
    "best_dice = 0.0\n",
    "start_epoch = 1\n",
    "end_epoch = 51\n",
    "\n",
    "# loading checkpoint when resuming\n",
    "# checkpoint = torch.load(\"./UNet3Dclassiccheckpoint1012.pt\")\n",
    "# net.load_state_dict(checkpoint['state_dict'])\n",
    "# opt.load_state_dict(checkpoint['optimizer'])\n",
    "# start_epoch = checkpoint['epoch']+ start_epoch\n",
    "# end_epoch = checkpoint['epoch'] + end_epoch\n",
    "# tr_losses = checkpoint['tr_losses']\n",
    "# val_losses = checkpoint['val_losses']\n",
    "# tr_dice_scores = checkpoint['tr_dice_scores']\n",
    "# dice_scores = checkpoint['dice_scores']\n",
    "\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for epoch in tqdm(range(start_epoch,end_epoch)):\n",
    "        tr_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        tr_dice = 0.0\n",
    "        dice = 0.0\n",
    "\n",
    "        # iteration through all training batches - forward pass, backpropagation, optimalization, performance evaluation\n",
    "        net.train()\n",
    "        print('\\n Epoch {}: First training batch loading'.format(epoch))\n",
    "        for img,lbl in tr_ds_dl:\n",
    "            img,lbl = img.to(device),lbl.to(device)\n",
    "\n",
    "            # calculating loss weights for cross entropy\n",
    "#             pos_weight = 1-(lbl.sum()/torch.numel(lbl))\n",
    "#             loss_weights = [1-pos_weight.item(),pos_weight.item()]\n",
    "#             if loss_weights == [0.0,1.0]: loss_weights = [0.01,0.99]\n",
    "#             loss_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(lbl.cpu().numpy().flatten()), y=lbl.cpu().numpy().flatten())\n",
    "#             loss_f = torch.nn.CrossEntropyLoss(weight = torch.Tensor(loss_weights).to(device))\n",
    "#             loss_f = CrossEntropyDiceLoss(weights=loss_weights)\n",
    "#             loss_f.weights = torch.cuda.FloatTensor(loss_weights)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            pred = net(img)\n",
    "\n",
    "            loss = loss_f(pred,lbl)\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            tr_loss+=loss.item()\n",
    "            pred = torch.argmax(torch.softmax(pred,dim = 1),dim = 1)\n",
    "            tr_dice += torchmetrics.functional.dice(pred,lbl,ignore_index = 0)\n",
    "\n",
    "        print('All training batches used')\n",
    "    \n",
    "        # showing improvement in detection through epochs\n",
    "        if epoch>0 and epoch % 20 == 0:\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            ax[0].imshow(torch.rot90(img[0,0,:,:,35].cpu()),cmap = 'gray')\n",
    "            ax[0].imshow(torch.rot90(pred[0,:,:,35].cpu()),alpha=0.5,cmap = 'copper')\n",
    "            ax[0].set_aspect(img.shape[2]/img.shape[3])\n",
    "            ax[0].set_title('Predikce')\n",
    "            ax[0].set_axis_off()\n",
    "            ax[1].imshow(torch.rot90(img[0,0,:,:,35].cpu()),cmap = 'gray')\n",
    "            ax[1].imshow(torch.rot90(lbl[0,:,:,35].cpu()),alpha=0.5,cmap = 'copper')\n",
    "            ax[1].set_aspect(img.shape[2]/img.shape[3])\n",
    "            ax[1].set_title('Zlatý standard')\n",
    "            ax[1].set_axis_off()\n",
    "            plt.show()\n",
    "            \n",
    "        # iteration through all validation batches - forward pass, performance evaluation\n",
    "        net.eval()\n",
    "        print('Epoch {}: First validation batch loading'.format(epoch))\n",
    "        with torch.no_grad():\n",
    "            for img,lbl in val_ds_dl:\n",
    "                img,lbl = img.to(device),lbl.to(device)\n",
    "                \n",
    "                # calculating loss weights for cross entropy\n",
    "#                 pos_weight = 1-(lbl.sum()/torch.numel(lbl))\n",
    "#                 loss_weights = [1-pos_weight.item(),pos_weight.item()]\n",
    "#                 if loss_weights == [0.0,1.0]: loss_weights = [0.01,0.99]\n",
    "#                 loss_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(lbl.cpu().numpy().flatten()), y=lbl.cpu().numpy().flatten())\n",
    "#                 loss_f = torch.nn.CrossEntropyLoss(weight = torch.Tensor(loss_weights).to(device))\n",
    "#                 loss_f = CrossEntropyDiceLoss(weights=loss_weights)\n",
    "#                 loss_f.weights = torch.cuda.FloatTensor(loss_weights)\n",
    "\n",
    "                pred=net(img)\n",
    "\n",
    "                loss=loss_f(pred,lbl)\n",
    "                \n",
    "                val_loss+=loss.item()\n",
    "                pred = torch.argmax(torch.softmax(pred,dim = 1), dim = 1)\n",
    "                dice += torchmetrics.functional.dice(pred,lbl,ignore_index = 0)\n",
    "\n",
    "        print('\\n All validation batches used')\n",
    "        tr_loss=tr_loss/len(tr_ds_dl)\n",
    "        val_loss=val_loss/len(val_ds_dl)\n",
    "        tr_dice = tr_dice/len(tr_ds_dl)\n",
    "        dice = dice/len(val_ds_dl)\n",
    "\n",
    "        tr_losses.append(tr_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        tr_dice_scores.append(tr_dice.detach().cpu().numpy())\n",
    "        dice_scores.append(dice.detach().cpu().numpy())\n",
    "\n",
    "        # scheduler.step()\n",
    "\n",
    "        # saving best model so far\n",
    "        if dice>best_dice:\n",
    "            best_net = copy.deepcopy(net)\n",
    "            best_dice = copy.deepcopy(dice)\n",
    "\n",
    "        print('Epoch {}; LR: {}; Train Loss: {:.4f}; Valid Loss: {:.4f}; Train Dice coeff: {:.4f}; Valid Dice coeff: {:.4f}'.format(epoch,opt.param_groups[0]['lr'],tr_loss,val_loss,tr_dice,dice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBhPGMI4TOzP"
   },
   "source": [
    "## Saving of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lv29MtfjVTy"
   },
   "source": [
    "Code for saving checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-fClc5F3jVT1"
   },
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': net.state_dict(),\n",
    "    'optimizer': opt.state_dict(),\n",
    "    'tr_losses': tr_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'tr_dice_scores': tr_dice_scores,\n",
    "    'dice_scores': dice_scores\n",
    "}\n",
    "torch.save(checkpoint, \"./3DUNetcheckpoint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMBfvt78jVT2"
   },
   "source": [
    "Code for saving the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CK3QP9yfTOzP"
   },
   "outputs": [],
   "source": [
    "torch.save(best_net,'./3DUNet.pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P3F6RVRjVT4"
   },
   "source": [
    "## Plot results of training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyyxGiNKjVT4",
    "outputId": "00011450-f5af-4e42-b1c1-dd1458605574"
   },
   "outputs": [],
   "source": [
    "plt.plot(tr_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.legend(['tr_loss','val_loss'])\n",
    "plt.xlabel(\"Epochy\")\n",
    "plt.ylabel(\"Kriteriální funkce\")\n",
    "plt.title('Kriteriální funkce - ATLAS R2.0 dataset')\n",
    "plt.show()\n",
    "plt.plot(tr_dice_scores)\n",
    "plt.plot(dice_scores)\n",
    "plt.legend(['tr_dice','val_dice'])\n",
    "plt.xlabel(\"Epochy\")\n",
    "plt.ylabel(\"DSC\")\n",
    "plt.title('DSC- ATLAS R2.0 dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GCUKjrt8uRx"
   },
   "source": [
    "## Evaluate one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWxLtrUq8vG5",
    "outputId": "cc79620a-4fbb-4dfe-c1a5-f02f5bcd3529"
   },
   "outputs": [],
   "source": [
    "# loading data for prediction and prediction\n",
    "image,label = ts_ds.__getitem__(3)\n",
    "image,label = image.to(device),label.to(device)\n",
    "net.eval()\n",
    "prediction = torch.argmax(torch.softmax(best_net(torch.unsqueeze(image,dim=0)),dim=1),dim=1)\n",
    "\n",
    "# plotting\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(torch.rot90(image[0,:,:,35].cpu()),cmap = 'gray')\n",
    "ax[0].imshow(torch.rot90(prediction[0,:,:,35].cpu()),alpha=0.5,cmap = 'copper')\n",
    "ax[0].set_aspect(image.shape[1]/image.shape[2])\n",
    "ax[0].set_title('Predikce')\n",
    "ax[0].set_axis_off()\n",
    "ax[1].imshow(torch.rot90(image[0,:,:,35].cpu()),cmap = 'gray')\n",
    "ax[1].imshow(torch.rot90(label[:,:,35].cpu()),alpha=0.5,cmap = 'copper')\n",
    "ax[1].set_aspect(image.shape[1]/image.shape[2])\n",
    "ax[1].set_title('Zlatý standard')\n",
    "ax[1].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbAJIJQ88jok"
   },
   "source": [
    "## Evaluate performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRtAS23l8kF1",
    "outputId": "912b634a-2901-4b1a-ebeb-10064d77d7f9"
   },
   "outputs": [],
   "source": [
    "ts_loss = 0.0\n",
    "ts_dice = 0.0\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for img,lbl in ts_ds_dl:\n",
    "        # loading of test batch\n",
    "        img,lbl = img.to(device),lbl.to(device)\n",
    "        \n",
    "        # calculating weight for cross entropy\n",
    "#         pos_weight = 1-(lbl.sum()/torch.numel(lbl))\n",
    "#         loss_weights = [1-pos_weight.item(),pos_weight.item()]\n",
    "#         loss_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(lbl.cpu().numpy().flatten()), y=lbl.cpu().numpy().flatten())\n",
    "#         loss_f = torch.nn.CrossEntropyLoss(weight = torch.Tensor(loss_weights).to(device))\n",
    "#         loss_f = CrossEntropyDiceLoss(weights=loss_weights)\n",
    "        \n",
    "        # prediction and performance estimation\n",
    "        pred=best_net(img)\n",
    "\n",
    "        loss=loss_f(pred,lbl)\n",
    "        ts_loss+=loss.item()\n",
    "        pred = torch.argmax(torch.softmax(pred,dim=1),dim=1)\n",
    "        ts_dice += torchmetrics.functional.dice(pred,lbl,ignore_index = 0)\n",
    "\n",
    "# final performance evaluation and printing out the result\n",
    "ts_loss=ts_loss/len(ts_ds_dl)\n",
    "ts_dice = ts_dice.detach().cpu().numpy()/len(ts_ds_dl)\n",
    "\n",
    "print('Test Loss: {:.4f}; Test Dice coeff: {:.4f}'.format(ts_loss,ts_dice))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
