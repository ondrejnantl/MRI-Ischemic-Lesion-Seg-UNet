{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M3zQEQy9Gsc"
   },
   "source": [
    "# 2D UNet training for stroke detection in MRI\n",
    "* one subject less than in ATLAS R2.0 - upload problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GV0gF_jW_TpJ"
   },
   "source": [
    "## Importing libraries, dividing annotated data into training, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5808,
     "status": "ok",
     "timestamp": 1668765056977,
     "user": {
      "displayName": "Ondřej Nantl",
      "userId": "00763901182080487698"
     },
     "user_tz": -60
    },
    "id": "tPGT2sV628Uz",
    "outputId": "e9d0335a-e697-4cf9-907f-2e00847b358e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from sklearn.utils.class_weight import compute_class_weight \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math,os,sys\n",
    "import random\n",
    "import copy\n",
    "from pdb import set_trace\n",
    "from tqdm import tqdm\n",
    "\n",
    "# then import my classes\n",
    "from loaders import ISLES_Dataset_2D\n",
    "from unet import UNet\n",
    "from resunet import ResUNet\n",
    "from loss_fcns import TverskyLoss,CrossEntropyDiceLoss,DiceLoss2D\n",
    "\n",
    "# %% device for pytorch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjpQUN39ndmw"
   },
   "source": [
    "## Code for generating the dataset lists\n",
    "This cell will produce 3 lists of folder names for training, validation and test datasets. It uses the whole ATLAS v2.0 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1667581057676,
     "user": {
      "displayName": "Ondřej Nantl",
      "userId": "00763901182080487698"
     },
     "user_tz": -60
    },
    "id": "1tUNBBRAndmx",
    "outputId": "757f80dd-e68a-4780-c470-ef739964f47a"
   },
   "outputs": [],
   "source": [
    "#%% get folder content of annotated data\n",
    "imgPath = r'/mnt/Data/ondrejnantl/DPData/train/derivatives/ATLAS'\n",
    "imgAnnot = sorted(os.listdir(imgPath))\n",
    "imgAnnot.remove(imgAnnot[0])\n",
    "\n",
    "valCount = math.floor(0.15*len(imgAnnot))\n",
    "testCount = math.floor(0.25*len(imgAnnot))\n",
    "trainCount = len(imgAnnot) - (valCount+testCount)\n",
    "\n",
    "randIdx = random.sample(imgAnnot,len(imgAnnot))\n",
    "\n",
    "# splitting subjects into training, validation and testing part\n",
    "valIdx = randIdx[0:(valCount)]\n",
    "testIdx = randIdx[valCount:(valCount+testCount)]\n",
    "trainIdx = randIdx[(valCount+testCount):len(imgAnnot)+1]\n",
    "\n",
    "print('ID lists created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KO-rL1GTk-Z"
   },
   "source": [
    "This cell will produce 3 lists of folder names for training, validation and test datasets. It uses only cohorts R002, R003, R004, R034 cohorts of ATLAS v2.0 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2J6Cr3KRTk-a",
    "outputId": "89261aea-b92e-41b6-b7c8-377a852461ce"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# get folder content of annotated data\n",
    "imgPath = r'/mnt/Data/ondrejnantl/DPData/train/derivatives/ATLAS'\n",
    "imgAnnot = sorted(os.listdir(imgPath))\n",
    "imgAnnot.remove(imgAnnot[0])\n",
    "\n",
    "# defining cohort to use for training\n",
    "cohorts = [\"^sub-r009.*$\"]\n",
    "selImgAnnot = []\n",
    "for it in cohorts:\n",
    "    r = re.compile(it)\n",
    "    tempList = list(filter(r.match, imgAnnot))\n",
    "    selImgAnnot = selImgAnnot + tempList\n",
    "\n",
    "valCount = math.floor(0.15*len(selImgAnnot))\n",
    "testCount = math.floor(0.25*len(selImgAnnot))\n",
    "trainCount = len(selImgAnnot) - (valCount+testCount)\n",
    "\n",
    "randIdx = random.sample(selImgAnnot,len(selImgAnnot))\n",
    "\n",
    "# splitting subjects into training, validation and testing part\n",
    "valIdx = randIdx[0:(valCount)]\n",
    "testIdx = randIdx[valCount:(valCount+testCount)]\n",
    "trainIdx = randIdx[(valCount+testCount):len(selImgAnnot)+1]\n",
    "\n",
    "print('ID lists created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Roonsfeindmx"
   },
   "source": [
    "## Load datasets' lists\n",
    "Loading object IDs from division of data - for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1668765062322,
     "user": {
      "displayName": "Ondřej Nantl",
      "userId": "00763901182080487698"
     },
     "user_tz": -60
    },
    "id": "SxYXe14Rndmx",
    "outputId": "c023c34e-3dd7-4e79-fc47-566782339ac8"
   },
   "outputs": [],
   "source": [
    "imgPath = r'/mnt/Data/ondrejnantl/DPData/train/derivatives/ATLAS'\n",
    "\n",
    "trainFd= open(r'/mnt/Data/ondrejnantl/DPData/trainNames.txt', 'r')\n",
    "trainIdx = trainFd.read()\n",
    "trainIdx = trainIdx.splitlines()\n",
    "trainFd.close()\n",
    "\n",
    "valFd= open(r'//mnt/Data/ondrejnantl/DPData/valNames.txt', 'r')\n",
    "valIdx = valFd.read()\n",
    "valIdx = valIdx.splitlines()\n",
    "valFd.close()\n",
    "\n",
    "testFd= open(r'/mnt/Data/ondrejnantl/DPData/testNames.txt', 'r')\n",
    "testIdx = testFd.read()\n",
    "testIdx = testIdx.splitlines()\n",
    "testFd.close()\n",
    "\n",
    "print('IDs loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70F4PowOTk-f"
   },
   "source": [
    "Load subject IDs for cohort R009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhQ_neYGTk-f",
    "outputId": "6c692c67-0280-483e-f95c-48350ebb05c4"
   },
   "outputs": [],
   "source": [
    "imgPath = r'/mnt/Data/ondrejnantl/DPData/train/derivatives/ATLAS'\n",
    "\n",
    "trainFd= open(r'/mnt/Data/ondrejnantl/DPData/trainNamesSP.txt', 'r')\n",
    "trainIdx = trainFd.read()\n",
    "trainIdx = trainIdx.splitlines()\n",
    "trainFd.close()\n",
    "\n",
    "valFd= open(r'//mnt/Data/ondrejnantl/DPData/valNamesSP.txt', 'r')\n",
    "valIdx = valFd.read()\n",
    "valIdx = valIdx.splitlines()\n",
    "valFd.close()\n",
    "\n",
    "testFd= open(r'/mnt/Data/ondrejnantl/DPData/testNamesSP.txt', 'r')\n",
    "testIdx = testFd.read()\n",
    "testIdx = testIdx.splitlines()\n",
    "testFd.close()\n",
    "\n",
    "print('IDs loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfrkERsu-KZ9"
   },
   "source": [
    "## Get dataset for 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1668765068630,
     "user": {
      "displayName": "Ondřej Nantl",
      "userId": "00763901182080487698"
     },
     "user_tz": -60
    },
    "id": "I9P-Kjp3-LAI",
    "outputId": "40005d89-d47a-4e79-ed31-d7614977d5de"
   },
   "outputs": [],
   "source": [
    "tr_ds = ISLES_Dataset_2D(\n",
    "    imgPath = imgPath,\n",
    "    imgAnnot = trainIdx[0:15],\n",
    "    sliceCount = 189,\n",
    "    cropped = True,\n",
    "    downsample = True\n",
    "    )\n",
    "tr_ds_dl = torch.utils.data.DataLoader(tr_ds,batch_size=64,shuffle=True)\n",
    "\n",
    "val_ds = ISLES_Dataset_2D(\n",
    "    imgPath = imgPath,\n",
    "    imgAnnot = valIdx[0:7],\n",
    "    sliceCount = 189,\n",
    "    cropped = True,\n",
    "    downsample = True\n",
    "    )\n",
    "val_ds_dl = torch.utils.data.DataLoader(val_ds,batch_size=64,shuffle=False)\n",
    "\n",
    "ts_ds = ISLES_Dataset_2D(\n",
    "    imgPath = imgPath,\n",
    "    imgAnnot = testIdx[0:10],\n",
    "    sliceCount = 189,\n",
    "    cropped = True,\n",
    "    downsample = True\n",
    "    )\n",
    "ts_ds_dl = torch.utils.data.DataLoader(ts_ds,batch_size=64,shuffle=False)\n",
    "\n",
    "print('Datasets and dataloaders created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqqS36SQAWHe"
   },
   "source": [
    "## Training the net and validation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hL4sNBVAWph",
    "outputId": "59cb0c6e-b2b1-46f9-e57a-6f11fa9f3f59"
   },
   "outputs": [],
   "source": [
    "#%% U-Net\n",
    "# net = ResUNet(1,2,filters=[32,64,128,256])\n",
    "net = UNet(1,2,filters=[32,64,128,256])\n",
    "net.to(device)\n",
    "\n",
    "#%% training and validation\n",
    "# loss_f = torch.nn.CrossEntropyLoss(weight = torch.Tensor([0.003, 0.997]).to(device))\n",
    "loss_f = DiceLoss2D()\n",
    "# loss_f = CrossEntropyDiceLoss(weights=[0.003,0.997])\n",
    "\n",
    "net.eval()\n",
    "opt = torch.optim.Adam(net.parameters(),lr=4e-5,weight_decay=5e-6)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=15, gamma=0.1)\n",
    "\n",
    "tr_losses = []\n",
    "val_losses = []\n",
    "dice_scores = []\n",
    "tr_dice_scores = []\n",
    "best_net = []\n",
    "learning_rates = []\n",
    "best_dice = 0.0\n",
    "start_epoch = 1\n",
    "end_epoch = 201\n",
    "\n",
    "# loading checkpoint when resuming\n",
    "# checkpoint = torch.load(\"./UNet2Dclassiccheckpoint1012.pt\")\n",
    "# net.load_state_dict(checkpoint['state_dict'])\n",
    "# opt.load_state_dict(checkpoint['optimizer'])\n",
    "# start_epoch = checkpoint['epoch']+ start_epoch\n",
    "# end_epoch = checkpoint['epoch'] + end_epoch\n",
    "# tr_losses = checkpoint['tr_losses']\n",
    "# val_losses = checkpoint['val_losses']\n",
    "# tr_dice_scores = checkpoint['tr_dice_scores']\n",
    "# dice_scores = checkpoint['dice_scores']\n",
    "\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for epoch in tqdm(range(start_epoch,end_epoch)):\n",
    "        tr_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        tr_dice = 0.0\n",
    "        dice = 0.0\n",
    "\n",
    "        # iteration through all training batches - forward pass, backpropagation, optimalization, performance evaluation\n",
    "        net.train()\n",
    "        print('\\n Epoch {}: First training batch loaded'.format(epoch))\n",
    "        for img,lbl in tr_ds_dl: \n",
    "            img,lbl = img.to(device),lbl.to(device)\n",
    "\n",
    "            # calculating of weights for Cross Entropy \n",
    "#             pos_weight = 1-(lbl.sum()/torch.numel(lbl))\n",
    "#             loss_weights = [1-pos_weight.item(),pos_weight.item()]\n",
    "#             if loss_weights == [0.0,1.0]: loss_weights = [0.01,0.99]\n",
    "#             loss_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(lbl.cpu().numpy().flatten()), y=lbl.cpu().numpy().flatten())\n",
    "#             loss_f = torch.nn.CrossEntropyLoss(weight = torch.Tensor(loss_weights).to(device))\n",
    "#             loss_f = CrossEntropyDiceLoss(weights=loss_weights)\n",
    "\n",
    "            pred = net(img)\n",
    "\n",
    "            loss = loss_f(pred,lbl)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            tr_loss+=loss.item()\n",
    "            pred = torch.argmax(torch.softmax(pred,dim=1),dim=1)\n",
    "            tr_dice += torchmetrics.functional.dice(pred,lbl,ignore_index = 0)\n",
    "\n",
    "        print('All training batches used')\n",
    "        \n",
    "        # showing improvement in detection through epochs\n",
    "        if epoch>0 and epoch % 5 == 0:\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            ax[0].imshow(torch.rot90(img[32,0,:,:].cpu()),cmap = 'gray')\n",
    "            ax[0].imshow(torch.rot90(pred[32,:,:].cpu()),alpha=0.5,cmap = 'copper')\n",
    "            ax[0].set_aspect(img.shape[2]/img.shape[3])\n",
    "            ax[0].set_title('Predikce')\n",
    "            ax[0].set_axis_off()\n",
    "            ax[1].imshow(torch.rot90(img[32,0,:,:].cpu()),cmap = 'gray')\n",
    "            ax[1].imshow(torch.rot90(lbl[32,:,:].cpu()),alpha=0.5,cmap = 'copper')\n",
    "            ax[1].set_aspect(img.shape[2]/img.shape[3])\n",
    "            ax[1].set_title('Zlatý standard')\n",
    "            ax[1].set_axis_off()\n",
    "            plt.show()\n",
    "    \n",
    "        # iteration through all validation batches - forward pass, performance estimation\n",
    "        net.eval()\n",
    "        print('Epoch {}: First validation batch loaded'.format(epoch))\n",
    "        with torch.no_grad():\n",
    "            for img,lbl in val_ds_dl:\n",
    "                img,lbl = img.to(device),lbl.to(device)\n",
    "\n",
    "                # calculating loss weights for Cross Entropy\n",
    "#                 pos_weight = 1-(lbl.sum()/torch.numel(lbl))\n",
    "#                 loss_weights = [1-pos_weight.item(),pos_weight.item()]\n",
    "#                 if loss_weights == [0.0,1.0]: loss_weights = [0.01,0.99]\n",
    "#                 loss_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(lbl.cpu().numpy().flatten()), y=lbl.cpu().numpy().flatten())\n",
    "#                 loss_f = torch.nn.CrossEntropyLoss(weight = torch.Tensor(loss_weights).to(device))\n",
    "#                 loss_f = CrossEntropyDiceLoss(weights=loss_weights)\n",
    "                \n",
    "                pred=net(img)\n",
    "                \n",
    "                loss=loss_f(pred,lbl)\n",
    "\n",
    "                val_loss+=loss.item()\n",
    "                pred = torch.argmax(torch.softmax(pred,dim=1),dim=1)\n",
    "                dice += torchmetrics.functional.dice(pred,lbl,ignore_index = 0)\n",
    "\n",
    "                if math.isnan(val_loss):\n",
    "                    print(\"Check variables\")\n",
    "                    set_trace()\n",
    "\n",
    "        print('\\n All validation batches used')\n",
    "        tr_loss=tr_loss/len(tr_ds_dl)\n",
    "        val_loss=val_loss/len(val_ds_dl)\n",
    "        tr_dice = tr_dice/len(tr_ds_dl)\n",
    "        dice = dice/len(val_ds_dl)\n",
    "\n",
    "        tr_losses.append(tr_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        tr_dice_scores.append(tr_dice.detach().cpu().numpy())\n",
    "        dice_scores.append(dice.detach().cpu().numpy())\n",
    "\n",
    "        # scheduler.step()\n",
    "\n",
    "        # saving the best model\n",
    "        if dice>best_dice:\n",
    "            best_net = copy.deepcopy(net)\n",
    "            best_dice = copy.deepcopy(dice)\n",
    "\n",
    "        print('Epoch {}; LR: {}; Train Loss: {:.4f}; Valid Loss: {:.4f}; Train Dice coeff: {:.4f}; Valid Dice coeff: {:.4f}'.format(epoch,opt.param_groups[0]['lr'],tr_loss,val_loss,tr_dice,dice))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ro4MZkxCTk-l"
   },
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtKC0wO5jX1y"
   },
   "source": [
    "Saving checkpoint for resuming the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DpCNgI-SjX1z"
   },
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'state_dict': net.state_dict(),\n",
    "    'optimizer': opt.state_dict(),\n",
    "    'tr_losses': tr_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'tr_dice_scores': tr_dice_scores,\n",
    "    'dice_scores': dice_scores\n",
    "}\n",
    "torch.save(checkpoint, \"./2DUNetcheckpoint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9G-aSJKjX10"
   },
   "source": [
    "Code for saving the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "w9N1jqW0Tk-l"
   },
   "outputs": [],
   "source": [
    "torch.save(best_net,'./2DUNet.pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWyDIZn-jX11"
   },
   "source": [
    "## Plot results of training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8r3jsRuMjX12",
    "outputId": "cca1964b-316c-41e5-8a8d-076701118d30"
   },
   "outputs": [],
   "source": [
    "plt.plot(tr_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.legend(['tr_loss','val_loss'])\n",
    "plt.xlabel(\"Epochy\")\n",
    "plt.ylabel(\"Kriteriální funkce\")\n",
    "plt.title('Kriteriální funkce - ATLAS R2.0 dataset')\n",
    "plt.show()\n",
    "plt.plot(tr_dice_scores)\n",
    "plt.plot(dice_scores)\n",
    "plt.legend(['tr_dice','val_dice'])\n",
    "plt.xlabel(\"Epochy\")\n",
    "plt.ylabel(\"DSC\")\n",
    "plt.title('DSC- ATLAS R2.0 dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GCUKjrt8uRx"
   },
   "source": [
    "## Evaluate one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWxLtrUq8vG5",
    "outputId": "ff308b49-ab67-4b8e-ed09-38d4d5b81450"
   },
   "outputs": [],
   "source": [
    "# loading data for prediction and prediction\n",
    "image,label = tr_ds.__getitem__(510)\n",
    "image,label = image.to(device),label.to(device)\n",
    "net.eval()\n",
    "prediction = torch.argmax(torch.softmax(net(torch.unsqueeze(image,dim=0)),dim=1),dim=1)\n",
    "\n",
    "# plotting detection\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(torch.rot90(image[0,:,:].cpu()),cmap = 'gray')\n",
    "ax[0].imshow(torch.rot90(prediction[0,:,:].cpu()),alpha=0.5,cmap = 'copper')\n",
    "ax[0].set_aspect(image.shape[1]/image.shape[2])\n",
    "ax[0].set_title('Predikce')\n",
    "ax[0].set_axis_off()\n",
    "ax[1].imshow(torch.rot90(image[0,:,:].cpu()),cmap = 'gray')\n",
    "ax[1].imshow(torch.rot90(label[:,:].cpu()),alpha=0.5,cmap = 'copper')\n",
    "ax[1].set_aspect(image.shape[1]/image.shape[2])\n",
    "ax[1].set_title('Zlatý standard')\n",
    "ax[1].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbAJIJQ88jok"
   },
   "source": [
    "## Evaluate performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRtAS23l8kF1",
    "outputId": "de0f76de-d0f5-4cbc-b9b4-e52938c09d6f"
   },
   "outputs": [],
   "source": [
    "ts_loss = 0.0\n",
    "ts_dice = 0.0\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img,lbl in ts_ds_dl:\n",
    "        # loading of test batch\n",
    "        img,lbl = img.to(device),lbl.to(device)\n",
    "        \n",
    "        # calculating loss weights for cross entropy\n",
    "#         pos_weight = 1-(lbl.sum()/torch.numel(lbl))\n",
    "#         loss_weights = [1-pos_weight.item(),pos_weight.item()]\n",
    "#         if loss_weights == [0.0,1.0]: loss_weights = [0.01,0.99]\n",
    "#         loss_f = torch.nn.CrossEntropyLoss(weight = torch.Tensor(loss_weights).to(device))\n",
    "#        loss_f = CrossEntropyDiceLoss(weights=loss_weights)\n",
    "        \n",
    "        # prediction\n",
    "        pred=net(img)\n",
    "        \n",
    "        # performance estimation\n",
    "        loss=loss_f(pred,lbl)\n",
    "        ts_loss+=loss.item()\n",
    "        pred = torch.argmax(torch.softmax(pred,dim=1),dim=1)\n",
    "        ts_dice += torchmetrics.functional.dice(pred,lbl,ignore_index = 0)\n",
    "\n",
    "ts_loss = ts_loss/len(ts_ds_dl)\n",
    "ts_dice = ts_dice.detach().cpu().numpy()/len(ts_ds_dl)\n",
    "\n",
    "print('Test Loss: {:.4f}; Test Dice coeff: {:.4f}'.format(ts_loss,ts_dice))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
